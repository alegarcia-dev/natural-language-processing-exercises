{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69db0e32-260b-4141-addd-c9f8eb475d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from acquire import get_codeup_blog, get_inshorts_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07452e0-4a4d-4b7c-8fd9-ee80a6ac8ece",
   "metadata": {},
   "source": [
    "# Data Preparation Exercises\n",
    "\n",
    "## 1\n",
    "\n",
    "Define a function named basic_clean. It should take in a string and apply some basic text cleaning to it:\n",
    "\n",
    "- Lowercase everything\n",
    "- Normalize unicode characters\n",
    "- Replace anything that is not a letter, number, whitespace or a single quote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2883bbc-7bf7-4fc0-baba-eb7881b9d2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A string to work with\n",
    "text = \"HERE is some text: α alpha  β beta | something * else. Someone's pencil.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9456e318-aeeb-4bfd-bf47-64a79bc6fc8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"here is some text: α alpha  β beta | something * else. someone's pencil.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lowercase everything\n",
    "cleaned = text.lower()\n",
    "cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc8546c2-6599-417c-ad36-0c551a124bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"here is some text:  alpha   beta | something * else. someone's pencil.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize unicode characters\n",
    "cleaned = unicodedata.normalize('NFKD', cleaned).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa9d792b-2656-42ef-82a8-787cfa32f1a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"here is some text  alpha   beta  something  else someone's pencil\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace special characters\n",
    "regexp = r\"[^a-z0-9'\\s]\"\n",
    "cleaned = re.sub(regexp, '', cleaned)\n",
    "cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "021d812d-c72f-4eef-8a7b-8b60b6fca05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's put it in a function\n",
    "\n",
    "def basic_clean(text):\n",
    "    text = text.lower()\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    regexp = r\"[^a-z0-9'\\s]\"\n",
    "    text = re.sub(regexp, '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adc4624a-9783-428d-b110-de0666f5d984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"here is some text  alpha   beta  something  else someone's pencil\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's test it\n",
    "basic_clean(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c294ae-16c3-4d76-b118-91e776280dd2",
   "metadata": {},
   "source": [
    "## 2\n",
    "\n",
    "Define a function named tokenize. It should take in a string and tokenize all the words in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9c7df5a-8ab8-4200-b9d0-328b00a2f4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function that will create a tokenizer object and tokenize the input\n",
    "\n",
    "def tokenize(text):\n",
    "    tokenizer = ToktokTokenizer()\n",
    "    return tokenizer.tokenize(text, return_str = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c492b4f-e696-460f-865f-e04d2f5d86fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"here is some text alpha beta something else someone ' s pencil\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's test it\n",
    "tokenize(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7366a4be-8a86-44d7-abd4-7d961aed6e4e",
   "metadata": {},
   "source": [
    "## 3\n",
    "\n",
    "Define a function named stem. It should accept some text and return the text after applying stemming to all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c90c6fae-f55e-4031-aa20-a38fb620cd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function that apply stemming to the input text\n",
    "\n",
    "def stem(text):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    stems = [ps.stem(word) for word in text.split()]\n",
    "    return ' '.join(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "645c98be-6fb1-4238-a571-6295fb008d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"here is some text alpha beta someth els someone' pencil\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's test it\n",
    "stem(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be28bbc-c511-4e8e-ab4c-7e79d9f56158",
   "metadata": {},
   "source": [
    "## 4\n",
    "\n",
    "Define a function named lemmatize. It should accept some text and return the text after applying lemmatization to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6699f1b-c867-4188-a73c-962c8eda0575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function that will apply lemmatization to the input text\n",
    "\n",
    "def lemmatize(text):\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    lemmas = [wnl.lemmatize(word) for word in text.split()]\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66b5048e-8d6f-4881-bcaa-778b2c74ca0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"here is some text alpha beta something else someone's pencil\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's test it\n",
    "lemmatize(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1305ec4-a2c8-4e86-830f-caa5990e6f3c",
   "metadata": {},
   "source": [
    "That didn't really change anything. Let's try a different string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc24ee76-ece4-4c6c-a427-ac62c49d50a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He study the principle of mathematical mumbo jumbo'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize(\"He studies the principles of mathematical mumbo jumbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200449da-7fb5-4b52-b0b3-a343fae4d3a9",
   "metadata": {},
   "source": [
    "## 5\n",
    "\n",
    "Define a function named remove_stopwords. It should accept some text and return the text after removing all the stopwords.\n",
    "\n",
    "This function should define two optional parameters, extra_words and exclude_words. These parameters should define any additional stop words to include, and any words that we don't want to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "845460fe-e8b9-4c1d-af2c-eb9d0f857d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try to add and remove words from the stopwords list\n",
    "\n",
    "stopword_list = stopwords.words('english')\n",
    "stopword_list[ : 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "34150d3a-97ae-4116-96e3-8c523de9f7b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'hubba',\n",
       " 'bubba']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First let's try adding some words\n",
    "\n",
    "extra_words = [\n",
    "    'hubba',\n",
    "    'bubba'\n",
    "]\n",
    "\n",
    "stopword_list += extra_words\n",
    "stopword_list[-10 : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "307e9bcc-9ca1-4537-ae52-e1fb41ceee01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " 'wouldn',\n",
       " 'hubba',\n",
       " 'bubba']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's try removing some words\n",
    "\n",
    "exclude_words = [\n",
    "    \"wouldn't\",\n",
    "    \"won't\"\n",
    "]\n",
    "\n",
    "[stopword_list.remove(word) for word in exclude_words]\n",
    "\n",
    "stopword_list[-10 : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "76b25a82-4303-4c7f-9541-3064f610d86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's create the function to remove all stopwords from the input text\n",
    "\n",
    "def remove_stopwords(text, extra_words = None, exclude_words = None):\n",
    "    stopword_list = stopwords.words('english')\n",
    "    \n",
    "    # We need to add in the extra checks if the parameters are None in order to make the \n",
    "    # parameters optional.\n",
    "    stopword_list = stopword_list + extra_words if extra_words is not None else stopword_list\n",
    "    [stopword_list.remove(word) for word in (exclude_words if exclude_words is not None else [])]\n",
    "    \n",
    "    text = [word for word in text.split() if word not in stopword_list]\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e74d2a2e-59a1-4f28-8814-5f8408ef4502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"text alpha beta something else someone's pencil\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's test it\n",
    "remove_stopwords(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "92dbe220-3858-4997-aaae-37e69eebff1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"here text something else someone's pencil\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(cleaned, extra_words = ['alpha', 'beta'], exclude_words = ['here'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39005956-7ee7-498b-bba4-81a97d4d1a13",
   "metadata": {},
   "source": [
    "## 6\n",
    "\n",
    "Use your data from the acquire to produce a dataframe of the news articles. Name the dataframe news_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af6b034-0806-4ad1-956c-f5e2243d7bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
